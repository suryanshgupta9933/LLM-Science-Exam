{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Falcon 7B Instruct Model using Perplexity Ranking"]},{"cell_type":"markdown","metadata":{},"source":["Installing Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install -q -U einops"]},{"cell_type":"markdown","metadata":{},"source":["Importing Dependencies"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-07-19T15:27:02.601745Z","iopub.status.busy":"2023-07-19T15:27:02.600902Z","iopub.status.idle":"2023-07-19T15:27:06.828746Z","shell.execute_reply":"2023-07-19T15:27:06.827667Z","shell.execute_reply.started":"2023-07-19T15:27:02.601699Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import torch\n","from torch import nn\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","import warnings\n","warnings.simplefilter(\"ignore\") # Ignore warnings"]},{"cell_type":"markdown","metadata":{},"source":["Define the model id"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_id = 'tiiuae/falcon-7b-instruct'"]},{"cell_type":"markdown","metadata":{},"source":["Load tokenizer from HuggingFace library using the given model id"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.pad_token = tokenizer.eos_token # set pad token as eos token"]},{"cell_type":"markdown","metadata":{},"source":["Load model from HuggingFace library using the given model id"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16,  # use float16 for model for lesser memory footprint\n","    device_map=\"auto\",  # automatically maps the model\n","    trust_remote_code=True,\n","    revision=\"main\"  # use main branch of the model\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Load the training and testing data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train = pd.read_csv('train.csv')\n","test = pd.read_csv('test.csv')"]},{"cell_type":"markdown","metadata":{},"source":["Define Perplexity class to compute perplexity of model's predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Perplexity(nn.Module):\n","    def __init__(self, reduce: bool = True):\n","        super().__init__()\n","        self.loss_fn = nn.CrossEntropyLoss(reduction='mean')  # using Cross Entropy Loss\n","        self.reduce = reduce\n","\n","    def forward(self, logits, labels):\n","        # Shift the labels and logits by one position to calculate loss\n","        shift_logits = logits[..., :-1, :].contiguous()\n","        shift_labels = labels[..., 1:].contiguous()\n","\n","        perplexity = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","        perplexity = perplexity.view(*shift_labels.size()[:-1])\n","\n","        if self.reduce:\n","            perplexity = torch.mean(perplexity)\n","\n","        return perplexity"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Move Perplexity object to GPU\n","perplexity_object = Perplexity().to(\"cuda\")"]},{"cell_type":"markdown","metadata":{},"source":["Function to calculate precision at k"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def calculate_precision_at_k(ranked_results, k):\n","    \"\"\"\n","    Calculates precision at k for ranked results.\n","    \"\"\"\n","    assert k <= len(ranked_results)\n","    assert k != 0\n","    return sum(int(x) for x in ranked_results[:k]) / k"]},{"cell_type":"markdown","metadata":{},"source":["Function to calculate mean average precision at 3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_MAP_at_3(predictions, true_answers):\n","    \"\"\"\n","    Calculates mean average precision at 3.\n","    \"\"\"\n","    num_users = len(predictions)\n","    map_at_3_score = 0.0\n","    for u in range(num_users):\n","        user_predictions = predictions[u]\n","        true_answer = true_items[u]\n","        user_results = [1 if prediction == true_answer else 0 for prediction in user_predictions]\n","        for k in range(min(len(user_predictions), 3)):\n","            map_at_3_score += precision_at_k(user_results, k+1) * user_results[k]\n","    return map_at_3_score / num_users"]},{"cell_type":"markdown","metadata":{},"source":["Function to compute perplexities"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def compute_perplexities(model, tokenizer, row):\n","    cols = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n","    perplexities = []\n","    # Prepare input for the model\n","    samples = [f\"{row['prompt']}</s>{row[col]}\" for col in cols]\n","    inputs = tokenizer(samples, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n","    # Get model's output\n","    outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n","    # Prepare labels\n","    labels = inputs[\"input_ids\"].clone()\n","    labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n","    # Calculate perplexity for each choice\n","    for j in range(len(cols)):\n","        perplexity = perp(outputs.logits[j].unsqueeze(0), labels[j].unsqueeze(0))\n","        perplexities.append(perplexity.detach().cpu())\n","\n","    del inputs, labels, outputs, perplexity\n","    torch.cuda.empty_cache()  # clear GPU cache\n","\n","    return np.array(perplexities)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["map_scores = []\n","all_predictions = []"]},{"cell_type":"markdown","metadata":{},"source":["For each row in the training data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for _, row in tqdm(train.iterrows(), total=len(train)):\n","    # Compute perplexities for choices\n","    perplexities = compute_perplexities(model, tokenizer, row)\n","    cols = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n","    # Sort predictions according to perplexities\n","    sorted_predictions = np.array(cols)[np.argsort(perplexities)]\n","    all_predictions.append([sorted_predictions])\n","    true_answer = [row.answer]\n","    # Calculate MAP score\n","    map_score = calculate_MAP_at_3([sorted_predictions], true_answer)\n","    map_scores.append(map_score)\n","    print(np.mean(map_scores))  # Print average MAP score"]},{"cell_type":"markdown","metadata":{},"source":["Mean Average Precision for the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean_map_score = np.mean(map_scores)\n","print(f\"Mean Average Precision (MAP) score is: {mean_map_score}\")"]},{"cell_type":"markdown","metadata":{},"source":["Load sample submission csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = pd.read_csv('sample_submission.csv')"]},{"cell_type":"markdown","metadata":{},"source":["Write predictions to csv file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submission[\"prediction\"] = [\" \".join(p[0][:3]) for p in preds]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.head()"]},{"cell_type":"markdown","metadata":{},"source":["Save the predictions to csv file"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
