{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Falcon 7B Instruct Model using Perplexity Ranking","metadata":{}},{"cell_type":"markdown","source":"## Installing Dependencies","metadata":{}},{"cell_type":"code","source":"# !pip install -q -U bitsandbytes\n# !pip install -q -U einops","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Dependencies","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nfrom transformers import AutoTokenizer, AutoModelForCausalLM","metadata":{"execution":{"iopub.status.busy":"2023-07-19T15:27:02.600902Z","iopub.execute_input":"2023-07-19T15:27:02.601745Z","iopub.status.idle":"2023-07-19T15:27:06.828746Z","shell.execute_reply.started":"2023-07-19T15:27:02.601699Z","shell.execute_reply":"2023-07-19T15:27:06.827667Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# import warnings\n# warnings.simplefilter(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = Path('/kaggle/input/kaggle-llm-science-exam')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = 'tiiuae/falcon-7b-instruct'\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"cuda:0\",\n    trust_remote_code=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    test = pd.read_csv(data_path / 'test.csv', index_col='id')\n    test[\"answer\"] = \"A\"\nelse:\n    test = pd.read_csv(data_path / 'train.csv', index_col='id')\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nclass Perplexity(nn.Module):\n    def __init__(self, reduce: bool = True):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.reduce = reduce\n\n    def forward(self, logits, labels):\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n\n        perplexity = []\n        for i in range(labels.shape[0]):\n            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n        perplexity = torch.stack(perplexity, dim=0)\n        #perplexity = torch.exp(perplexity)\n        if self.reduce:\n            perplexity = torch.mean(perplexity)\n        return perplexity \n    \nperp = Perplexity()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef precision_at_k(r, k):\n    \"\"\"Precision at k\"\"\"\n    assert k <= len(r)\n    assert k != 0\n    return sum(int(x) for x in r[:k]) / k\n\ndef MAP_at_3(predictions, true_items):\n    \"\"\"Score is mean average precision at 3\"\"\"\n    U = len(predictions)\n    map_at_3 = 0.0\n    for u in range(U):\n        user_preds = predictions[u]\n        user_true = true_items[u]\n        user_results = [1 if item == user_true else 0 for item in user_preds]\n        for k in range(min(len(user_preds), 3)):\n            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n    return map_at_3 / U\n\nmaps = []\npreds = []\nfor idx, row in tqdm(test.iterrows(), total=len(test)):\n        \n    \n    with torch.no_grad():\n        cols = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n        perps = []\n        samples = []\n        for col in cols:\n            samples.append(\"<|prompt|>\"+row[\"prompt\"]+\"</s><|answer|>\"+row[col])\n        inputs = tokenizer(samples, return_tensors=\"pt\", add_special_tokens=False, padding=True, truncation=True).to(\"cuda\")\n\n        output = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n        output = output.logits\n        labels = inputs[\"input_ids\"]\n        labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n        for j in range(len(cols)):\n            p = perp(output[j].unsqueeze(0), labels[j].unsqueeze(0))\n            perps.append(p.detach().cpu())\n            \n        del inputs\n        del labels\n        del output\n        del p\n\n    perps = np.array(perps)\n        \n    predictions = [np.array(cols)[np.argsort(perps)]]\n    preds.append(predictions)\n    tp = [row.answer]\n    map = MAP_at_3(predictions, tp)\n    maps.append(map)\n#     print(np.mean(maps))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(maps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(data_path / 'sample_submission.csv')\n\nsubmission[\"prediction\"] = [\" \".join(p[0][:3]) for p in preds]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}