{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Falcon 7B Instruct Model using Perplexity Ranking"]},{"cell_type":"markdown","metadata":{},"source":["Installing Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-19T18:22:51.326285Z","iopub.status.busy":"2023-07-19T18:22:51.325869Z","iopub.status.idle":"2023-07-19T18:22:51.332569Z","shell.execute_reply":"2023-07-19T18:22:51.331291Z","shell.execute_reply.started":"2023-07-19T18:22:51.326228Z"},"trusted":true},"outputs":[],"source":["!pip install -q -U einops"]},{"cell_type":"markdown","metadata":{},"source":["Importing Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-19T18:22:51.334803Z","iopub.status.busy":"2023-07-19T18:22:51.334433Z","iopub.status.idle":"2023-07-19T18:22:55.038825Z","shell.execute_reply":"2023-07-19T18:22:55.037638Z","shell.execute_reply.started":"2023-07-19T18:22:51.334769Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import torch\n","from torch import nn\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","import warnings\n","warnings.simplefilter(\"ignore\") # Ignore warnings"]},{"cell_type":"markdown","metadata":{},"source":["Define the model id"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-19T18:22:55.041149Z","iopub.status.busy":"2023-07-19T18:22:55.040419Z","iopub.status.idle":"2023-07-19T18:22:55.048599Z","shell.execute_reply":"2023-07-19T18:22:55.046121Z","shell.execute_reply.started":"2023-07-19T18:22:55.041104Z"},"trusted":true},"outputs":[],"source":["model_id = 'tiiuae/falcon-7b-instruct'"]},{"cell_type":"markdown","metadata":{},"source":["Load tokenizer from HuggingFace library using the given model id"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-19T18:22:55.054005Z","iopub.status.busy":"2023-07-19T18:22:55.052756Z","iopub.status.idle":"2023-07-19T18:22:55.474251Z","shell.execute_reply":"2023-07-19T18:22:55.473071Z","shell.execute_reply.started":"2023-07-19T18:22:55.053961Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id)\n","tokenizer.pad_token = tokenizer.eos_token # set pad token as eos token"]},{"cell_type":"markdown","metadata":{},"source":["Load model from HuggingFace library using the given model id"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-19T18:22:55.482874Z","iopub.status.busy":"2023-07-19T18:22:55.479649Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.bfloat16,  # use float16 for model for lesser memory footprint\n","    device_map=\"auto\",  # automatically maps the model\n","    trust_remote_code=True,\n","    revision=\"main\"  # use main branch of the model\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Load the training and testing data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/train.csv')\n","test = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')"]},{"cell_type":"markdown","metadata":{},"source":["Define Perplexity class"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Perplexity(nn.Module):\n","    \"\"\"\n","    A class used to compute the perplexity of model's predictions\n","    \"\"\"\n","    def __init__(self, reduce: bool = True):\n","        super().__init__()\n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.reduce = reduce\n","\n","    def forward(self, logits, labels):\n","        # Shift the labels and logits by one position to calculate loss\n","        shift_logits = logits[..., :-1, :].contiguous()\n","        shift_labels = labels[..., 1:].contiguous()\n","\n","        # Compute loss for each item in the batch\n","        perplexity = [self.loss_fn(shift_logits[i], shift_labels[i]) for i in range(labels.shape[0])]\n","        perplexity = torch.stack(perplexity, dim=0)\n","\n","        # Compute mean if reduce is set to True\n","        if self.reduce:\n","            perplexity = torch.mean(perplexity)\n","\n","        return perplexity"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Move Perplexity object to GPU\n","perplexity_calculator = Perplexity().to(\"cuda\")"]},{"cell_type":"markdown","metadata":{},"source":["Function to calculate precision at k"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def calculate_precision_at_k(r, k):\n","    \"\"\"\n","    Calculates precision at k for ranked results.\n","    \"\"\"\n","    assert k <= len(r)\n","    assert k != 0\n","    return sum(int(x) for x in r[:k]) / k"]},{"cell_type":"markdown","metadata":{},"source":["Function to calculate mean average precision at 3"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def calculate_MAP_at_3(predictions, true_items):\n","    \"\"\"\n","    Calculates mean average precision at 3.\n","    \"\"\"\n","    U = len(predictions)\n","    map_at_3 = 0.0\n","\n","    for u in range(U):\n","        user_preds = predictions[u]\n","        user_true = true_items[u]\n","        user_results = [1 if item == user_true else 0 for item in user_preds]\n","\n","        for k in range(min(len(user_preds), 3)):\n","            map_at_3 += calculate_precision_at_k(user_results, k+1) * user_results[k]\n","\n","    return map_at_3 / U"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["map_scores = []\n","predictions_list = []"]},{"cell_type":"markdown","metadata":{},"source":["For each row in the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for idx, row in tqdm(train.iterrows(), total=len(train)):\n","    with torch.no_grad():\n","        cols = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n","        perplexities = []\n","        samples = [f\"{row['prompt']}</s>{row[col]}\" for col in cols]\n","\n","        # Prepare input for the model\n","        inputs = tokenizer(\n","            samples, \n","            return_tensors=\"pt\", \n","            add_special_tokens=False, \n","            padding=True, \n","            truncation=True\n","        ).to(\"cuda\")\n","\n","        # Get model's output\n","        output = model(\n","            input_ids=inputs[\"input_ids\"], \n","            attention_mask=inputs[\"attention_mask\"]\n","        ).logits\n","\n","        # Prepare labels\n","        labels = inputs[\"input_ids\"]\n","        labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n","\n","        # Calculate perplexity for each choice\n","        for j in range(len(cols)):\n","            perplexity = perplexity_calculator(output[j].unsqueeze(0), labels[j].unsqueeze(0))\n","            perplexities.append(perplexity.detach().cpu())\n","\n","        # Delete tensors to free up GPU memory\n","        del inputs, labels, output, perplexity\n","\n","    perplexities = np.array(perplexities)\n","\n","    # Sort predictions according to perplexities\n","    sorted_predictions = np.array(cols)[np.argsort(perplexities)]\n","    predictions_list.append([sorted_predictions])\n","\n","    true_answer = [row.answer]\n","    map_score = calculate_MAP_at_3([sorted_predictions], true_answer)\n","    map_scores.append(map_score)"]},{"cell_type":"markdown","metadata":{},"source":["Mean Average Precision for the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean_map_score = np.mean(map_scores)\n","print(f\"Mean Average Precision (MAP) score is: {mean_map_score}\")"]},{"cell_type":"markdown","metadata":{},"source":["Load sample submission csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = pd.read_csv('sample_submission.csv')"]},{"cell_type":"markdown","metadata":{},"source":["Write predictions to csv file"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission[\"prediction\"] = [\" \".join(p[0][:3]) for p in preds]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.head()"]},{"cell_type":"markdown","metadata":{},"source":["Save the predictions to csv file"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
